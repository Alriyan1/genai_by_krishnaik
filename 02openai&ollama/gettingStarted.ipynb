{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb98412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['LANGSMITH_API_KEY']='true'\n",
    "os.environ['LANGSMITH_API_KEY']=getpass.getpass('********************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8923c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(\n",
    "            api_key = \"***********************\",\n",
    "            model = \"llama-3.3-70b-versatile\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e484e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model='gpt-4o')\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37db1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x00000109A7B31FD0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000109A7B32CF0> model_name='llama-3.3-70b-versatile' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke('what is generative AI')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12048e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a type of artificial intelligence (AI) that is capable of generating new, original content, such as images, videos, music, text, or even entire datasets. This is in contrast to traditional AI, which is typically designed to perform specific tasks, such as classification, regression, or optimization.\\n\\nGenerative AI models use complex algorithms and neural networks to learn patterns and relationships within a given dataset, and then use this knowledge to generate new, synthetic data that is similar in style and structure to the original data. This can be useful for a wide range of applications, including:\\n\\n1. **Art and design**: Generative AI can be used to create new, original artwork, such as paintings, sculptures, or music compositions.\\n2. **Data augmentation**: Generative AI can be used to generate new training data for machine learning models, which can help to improve their performance and accuracy.\\n3. **Content creation**: Generative AI can be used to generate new text, such as articles, stories, or dialogue, or to create new videos, such as animations or special effects.\\n4. **Simulation and modeling**: Generative AI can be used to simulate complex systems, such as weather patterns, traffic flow, or population dynamics.\\n5. **Personalization**: Generative AI can be used to create personalized content, such as product recommendations or customized advertising.\\n\\nSome common techniques used in generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networks that work together to generate new data. One network generates new data, while the other network evaluates the generated data and provides feedback to the first network.\\n2. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that can be used to generate new data by learning a probabilistic representation of the input data.\\n3. **Recurrent Neural Networks (RNNs)**: RNNs are a type of neural network that can be used to generate sequential data, such as text or music.\\n4. **Transformers**: Transformers are a type of neural network that can be used to generate text and other sequential data.\\n\\nSome examples of generative AI applications include:\\n\\n1. **Deepfakes**: Deepfakes are AI-generated videos or images that are designed to mimic the appearance and voice of a real person.\\n2. **AI-generated music**: AI-generated music is music that is created using generative AI algorithms, such as Amper Music or AIVA.\\n3. **Chatbots**: Chatbots are computer programs that use generative AI to generate human-like responses to user input.\\n4. **AI-generated art**: AI-generated art is art that is created using generative AI algorithms, such as the Next Rembrandt project or the AI-generated portrait that sold at Christie's auction house.\\n\\nOverall, generative AI has the potential to revolutionize a wide range of industries and applications, from art and entertainment to science and technology.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 606, 'prompt_tokens': 40, 'total_tokens': 646, 'completion_time': 2.203636364, 'prompt_time': 0.003375951, 'queue_time': 0.054242509, 'total_time': 2.207012315}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None} id='run--f90337fb-9e84-4dcf-85e3-d15b71cd2a40-0' usage_metadata={'input_tokens': 40, 'output_tokens': 606, 'total_tokens': 646}\n"
     ]
    }
   ],
   "source": [
    "output=model.invoke('what is generative AI')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1c03ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a AI engineer. Provide me answer based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a AI engineer. Provide me answer based on the questions'),\n",
    "        ('user','{input}')\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e958da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Langsmith Library Overview**\\n=====================================\\n\\nThe Langsmith library is an open-source Python library developed by Meta AI, designed to simplify and accelerate the development of natural language processing (NLP) models. It provides a set of tools and interfaces for building, training, and deploying NLP models, with a focus on ease of use, flexibility, and performance.\\n\\n**Key Features**\\n---------------\\n\\n1. **Modular Architecture**: Langsmith is built around a modular architecture, allowing developers to easily swap out or combine different components, such as tokenizers, embedders, and decoders.\\n2. **Pre-trained Models**: The library provides access to a range of pre-trained models, including popular transformers like BERT, RoBERTa, and XLNet.\\n3. **Customizable**: Langsmith allows developers to customize their models by modifying hyperparameters, adding new layers, or using different optimization algorithms.\\n4. **Multi-Task Learning**: The library supports multi-task learning, enabling developers to train a single model on multiple NLP tasks, such as sentiment analysis, question answering, and text classification.\\n5. **Efficient Training**: Langsmith includes optimized training loops and batch processing, making it possible to train large models on limited hardware.\\n\\n**Use Cases**\\n-------------\\n\\n1. **Text Classification**: Langsmith can be used for text classification tasks, such as sentiment analysis, spam detection, or topic modeling.\\n2. **Question Answering**: The library provides tools for building question answering models, including support for popular datasets like SQuAD and Natural Questions.\\n3. **Language Translation**: Langsmith can be used for machine translation tasks, including support for popular datasets like WMT and IWSLT.\\n4. **Text Generation**: The library includes tools for text generation tasks, such as language modeling, text summarization, and dialogue generation.\\n\\n**Example Code**\\n---------------\\n\\nHere\\'s an example of how to use Langsmith to fine-tune a pre-trained BERT model for a text classification task:\\n```python\\nimport langsmith as ls\\n\\n# Load pre-trained BERT model\\nmodel = ls.load_pretrained_model(\"bert-base-uncased\")\\n\\n# Load dataset\\ndataset = ls.load_dataset(\"my_dataset\")\\n\\n# Fine-tune model\\nmodel = ls.fine_tune_model(model, dataset, num_epochs=5)\\n\\n# Evaluate model\\naccuracy = ls.evaluate_model(model, dataset)\\nprint(f\"Accuracy: {accuracy:.4f}\")\\n```\\nNote that this is just a brief overview of the Langsmith library, and there are many more features and capabilities to explore. If you have specific questions or need further guidance, feel free to ask!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 536, 'prompt_tokens': 57, 'total_tokens': 593, 'completion_time': 2.121833179, 'prompt_time': 0.003181884, 'queue_time': 0.053396281, 'total_time': 2.125015063}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None} id='run--82bdbf72-d847-4f16-af18-8494119697c5-0' usage_metadata={'input_tokens': 57, 'output_tokens': 536, 'total_tokens': 593}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model\n",
    "response = chain.invoke({'input':'can you tell me about langsmith library?'})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923cf101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60016d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain Library Overview**\n",
      "================================\n",
      "\n",
      "LangChain is an open-source Python library designed to simplify the process of building and interacting with large language models (LLMs). It provides a unified interface for various LLMs, allowing developers to focus on application development rather than model implementation details.\n",
      "\n",
      "**Key Features**\n",
      "---------------\n",
      "\n",
      "1. **Model Agnostic**: LangChain supports multiple LLMs, including LLaMA, PaLM, and BERT, among others.\n",
      "2. **Simple Interface**: The library provides a simple, Pythonic API for interacting with LLMs, making it easy to integrate them into applications.\n",
      "3. **Modular Architecture**: LangChain's modular design allows developers to easily swap out different models, fine-tune models, or add custom functionality.\n",
      "4. **Extensive Documentation**: The library comes with comprehensive documentation, including tutorials, examples, and API references.\n",
      "\n",
      "**Use Cases**\n",
      "------------\n",
      "\n",
      "1. **Text Generation**: LangChain can be used for text generation tasks, such as chatbots, content creation, or language translation.\n",
      "2. **Question Answering**: The library can be employed for question answering tasks, like building QA systems or chatbots.\n",
      "3. **Text Classification**: LangChain can be used for text classification tasks, such as sentiment analysis or spam detection.\n",
      "4. **Conversational AI**: The library is suitable for building conversational AI applications, including voice assistants or customer support chatbots.\n",
      "\n",
      "**Installation and Usage**\n",
      "-------------------------\n",
      "\n",
      "To install LangChain, run the following command:\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "Once installed, you can import the library and start using it in your Python application:\n",
      "```python\n",
      "import langchain\n",
      "\n",
      "# Initialize the LLM\n",
      "llm = langchain.LLM(\"llama\")\n",
      "\n",
      "# Use the LLM for text generation\n",
      "output = llm.generate(\"Hello, how are you?\")\n",
      "print(output)\n",
      "```\n",
      "**Example Use Case: Building a Simple Chatbot**\n",
      "---------------------------------------------\n",
      "\n",
      "Here's an example of using LangChain to build a simple chatbot:\n",
      "```python\n",
      "import langchain\n",
      "\n",
      "# Initialize the LLM\n",
      "llm = langchain.LLM(\"llama\")\n",
      "\n",
      "def chatbot(input_text):\n",
      "    # Generate a response using the LLM\n",
      "    response = llm.generate(input_text)\n",
      "    return response\n",
      "\n",
      "# Test the chatbot\n",
      "input_text = \"Hello, how are you?\"\n",
      "response = chatbot(input_text)\n",
      "print(response)\n",
      "```\n",
      "This is just a brief introduction to the LangChain library. If you have any specific questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({'input':'can you tell me about langchain library'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646f435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
